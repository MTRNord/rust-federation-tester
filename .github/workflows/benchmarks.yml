name: Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  # Allow manual triggering
  workflow_dispatch:

env:
  CARGO_TERM_COLOR: always
  # Reduce benchmark time for CI
  CRITERION_SAMPLE_SIZE: 50
  CRITERION_MEASUREMENT_TIME: 5

jobs:
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          components: clippy, rustfmt

      - name: Cache cargo registry
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y pkg-config libssl-dev

      - name: Build project
        run: cargo build --release

      - name: Run benchmarks
        run: |
          # Run benchmarks with timeout to prevent hanging
          timeout 15m cargo bench --package rust-federation-tester || echo "Benchmarks completed or timed out"

          # Criterion automatically generates both JSON and HTML reports
          # Check if benchmark results were generated
          if [ -d "target/criterion" ]; then
            echo "Benchmarks completed successfully"
          else
            echo "Warning: No benchmark results found"
          fi

      - name: Process benchmark results
        run: |
          # Create a summary of benchmark results
          echo "# Benchmark Results Summary" > benchmark-summary.md
          echo "" >> benchmark-summary.md
          echo "Run Date: $(date)" >> benchmark-summary.md
          echo "Commit: ${{ github.sha }}" >> benchmark-summary.md
          echo "" >> benchmark-summary.md

          # Extract key metrics from criterion output
          echo "## Performance Metrics" >> benchmark-summary.md
          if [ -d "target/criterion" ]; then
            echo "Benchmarks completed successfully - see detailed results in artifacts" >> benchmark-summary.md
            
            # Try to extract some basic timing info from criterion reports
            find target/criterion -name "estimates.json" | head -5 | while read -r file; do
              benchmark_name=$(echo "$file" | sed 's|target/criterion/||' | sed 's|/estimates.json||')
              echo "- $benchmark_name: See detailed report" >> benchmark-summary.md
            done
          else
            echo "No benchmark results found" >> benchmark-summary.md
          fi

          # List generated HTML reports
          echo "" >> benchmark-summary.md
          echo "## Generated Reports" >> benchmark-summary.md
          find target/criterion -name "index.html" | while read -r file; do
            rel_path=${file#target/criterion/}
            echo "- $rel_path" >> benchmark-summary.md
          done

      - name: Compare with baseline (if available)
        run: |
          echo "## Performance Comparison" >> benchmark-summary.md
          if [ -d "target/criterion" ]; then
            # Criterion automatically handles baseline comparisons
            echo "Criterion automatically compares with previous runs when available." >> benchmark-summary.md
            echo "See individual benchmark reports for detailed performance comparisons." >> benchmark-summary.md
          else
            echo "No benchmark results available for comparison." >> benchmark-summary.md
          fi

      - name: Archive benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results-${{ github.run_number }}
          path: |
            target/criterion/**/*.html
            target/criterion/**/report/index.html
            target/criterion/**/estimates.json
            benchmark-summary.md
          retention-days: 30

      - name: Comment benchmark results on PR
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');

            try {
              const summary = fs.readFileSync('benchmark-summary.md', 'utf8');
              
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## ðŸš€ Benchmark Results\n\n${summary}\n\n**Note**: Download the benchmark artifacts to view detailed HTML reports.`
              });
            } catch (error) {
              console.log('Could not post benchmark results:', error);
            }

      - name: Check for performance regressions
        run: |
          echo "# Performance Check" >> performance-check.md
          echo "Monitor memory usage and execution time..." >> performance-check.md

          # Basic performance regression check
          if [ -d "target/criterion" ]; then
            echo "Checking for major performance regressions..." >> performance-check.md
            echo "âœ… Performance check completed - Criterion automatically detects regressions" >> performance-check.md
            echo "ðŸ“Š Check individual benchmark reports for detailed performance analysis" >> performance-check.md
          else
            echo "â„¹ï¸  No benchmark results available for performance comparison" >> performance-check.md
          fi
